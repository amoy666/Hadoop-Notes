hdfs的四大机制，两大核心
hdfs提供的是高容错性的分布式的数据存储方案
    hadoop集群的启动顺序：
    namenode --> datanode -->secondarynamenode
    4大机制：
        1、心跳机制
            集群节点之间必须做时间同步
            namenode是集群的老大,负责集群上任务的分工,如果要进行分工,则必须知道各个从节点的存活状况
            amende怎么知道的?通过 datanode定期的向 namenode发送心跳报告
            datanode会每隔3s向 namenode发送一次心跳报告,目的可就是告诉 namenode自己的存活状况
            心跳间隔时间可以通过hdfs-site.xml配置文件设置，如下所示。
            <property>
              <name>dfs.heartbeat.interval</name>
              <value>3</value>
              <description>Determines datanode heartbeat interval in seconds.</description>
            </property>
            namenode什么时候会判断datanode死了？
                datanode每隔3s向namenode发送一次心跳报告，当namenode连续10次没有收到datanode的心跳报告则认为这个datanode可能死了，并没有断定死了，这个时候namenode会主动向datanode发送一次检查，发送一次检查的时间是5分钟，namenode给datanode2次机会，如果1次检查没有返回信息，这时候会再进行一次检查，如果再次获取不到datanode的信息，这时候才会判定该datanode死亡了。

                也就是说namenode最终判断datanode死亡需要10*3s+2*5min=630s
                也就是说namenode在连续630秒中没有得到datanode的信息才会认为datanode死亡了

        2、安全模式
            集群启动的时候namenode需要做哪些事情：
                元数据：1.抽象目录树，
                        2.数据和块的映射关系，
                        3.数据块存储的位置信息
                元数据存储位置：
                    内存：读写快，但是一旦关机就会造成数据丢失，所以元数据既在内存存储，又在磁盘存储，内存中存储的元数据
                    磁盘：元数据如果存储在磁盘，那么每次进行文件读写的时候，都会进行磁盘读写，必然会造成磁盘读写的性能比较低，所以文件的读写的元数据不应该在磁盘读写
                当集群第一次启动的时候，首先会将磁盘元数据加载到内存中，如果磁盘中的元数据过大会造成加载到内存的时间过长，所以磁盘中的元数据只存储了1，2两部分，namenode的内存元数据的3是什么时候获取的。是通过datanode的心跳报告获取的，集群在启动的时候namenode会接收datanode的心跳报告，这个心跳报告中还包含数据块的存储位置信息，这时候namenode就可以获取datanode的数据块的存储状况.

                集群启动之后：
                    1.namenode启动的时候namenode将元数据加载到内存中
                    2.namenode接收datanode的心跳报告，目的：1）获取datanode的存活状况，2）获取块的存储信息
                    3.启动secondarynamenode
                集群在执行这个过程的时候不允许外界对集群进行操作的，这个时候集群处于安全模式

                也就是说集群处于安全模式的时候在加载元数据和获取datanode的心跳报告

                如果集群处于维护或升级时候也可以手动将集群设置安全模式，命令为：
                    hdfs dfsadmin -safemode [enter | leave | get | wait]
                    enter:进入安全模式
                    leave：退出安全模式
                    get：获取安全模式的状态是开启（ON）还是关闭（OFF）
                    wait：等待自行退出安全模式
                安全模式下用户可以做的事情：（不修改元数据的操作）
                    ls查询
                    cat查看
                    get下载
                安全模式下用户不能做的事情：（修改元数据的操作）
                    创建目录
                    put上传
                    重命名

        3、机架策略：副本存放机制
            副本存放策略：
                1.第一个副本一般存储在客户端所在的节点上
                2.第二个副本存储在和第一个副本不同的机架上的任意一个节点的时候
                    原因：防止同一个机架断电，数据访问不到
                3.第三个副本存储在和第一个副本相同机架上的不同的节点上
                    原因：在风险相同的情况下，优先选择网络传输少的
            真实生产中需要手动配置机架策略
            真实生产中我么可以自定义机架策略
                不同节点
                不同机器
                不同数据中心

        4、负载均衡
            什么是负载均衡：每个节点上存储的数据百分比差不多   能力越大责任越大
            在文件上传的时候会优先选择客户端所在的节点，如果习惯性使用同一个客户端会造成客户端所在节点存储的数据比较多

            集群会有一个自动的负载均衡的操作，只不过这种负载均衡的操作比较慢
            <property>
              <name>dfs.datanode.balance.bandwidthPerSec</name>
              <value>1048576</value> --> 1MB
              <description>
                    Specifies the maximum amount of bandwidth that each datanode
                    can utilize for the balancing purpose in term of
                    the number of bytes per second.
              </description>
            </property>
            这个参数是限制负载均衡的带块的，默认是1M/s  在集群空闲的情况下
            集群自动的负载均衡对于小规模是可以的
            如果集群规模特别大的时候，会花费很长的时间，这个时候需要手动负载均衡
            start-balancer.sh 但是这个命令也不会立即执行   等待Hadoop集群空闲的时候才执行

            存在绝对的均衡?不存在的
            所以我们在做手动负载均衡的时候可以指定一个参数
            start-balancer.sh -t 10%
            -t 10%指的是任意两个节点之间的存储百分比不超过10%则认为已经达到了负载均衡
            负载均衡什么时候发生的概率比较高:集群中添加新的节点的时候



    作为一个文件系统2大核心功能
        1、上传
        2、下载
            hadoop fs -get
            1.客户端向 namenode发送文件下载请求
            2. namenode在自己的元数据库中进行查询,如果查询到则会返回给客户端数据的块及副本存储节点
                blk 1: hadoop01 hadoop02 hadoop04
                blk 2: hadoop01 hadoop03 hadoop04
            如果查询不到则会报错
            3.客户端拿到了数据块的存储节点，就会进行第一个数据块的下载
                进行下载的时候也是就近原则
            4.第一个快下载成功后会生成一个CRC文件，和上传的时候。meta文件进行文件完整度校验（校验的是起始偏移量和末尾偏移量之间的内容）
                如果校验通过，则认为第一个块下载成功
            5.进行第二个快的下载重复下载动作
            6.所有的块下载成功后向namenode发送数据下载成功响应

            文件下载中如果产生异常:数据块的某一个节点读取不到数据这个时候会向 namenode进行汇报, namenode就会对这个节点做一个标记,标记这个节点可能是问题节点，接着读取这个快存储的其他节点


        3、元数据管理
        元数据:抽象目录树数据和块的映射数据块的存储节点
        内存:1,2,3
        磁盘:1,2

        /home/admin/data/hadoopdata目录下有3个文件夹:
        data:数据的真实存储目录 datanode存储数据的存储目录
        name: namenode存储元数据的目录
        nm-1oca1-dir:本地缓存    hdfs的本地缓存

        元数据存储目录下的文件分为4类：
        edits_0000000000000000001-0000000000000000002
        edits_0000000000000000003-0000000000000000006
        edits_0000000000000000007-0000000000000001813
        edits_0000000000000001814-0000000000000001814
        edits_0000000000000001815-0000000000000001816
        edits_0000000000000001817-0000000000000001821
        edits_0000000000000001822-0000000000000001859
        edits_0000000000000001860-0000000000000001861
        edits_0000000000000001862-0000000000000001863
        edits_0000000000000001864-0000000000000001907
        edits_0000000000000001908-0000000000000001918
        edits_0000000000000001919-0000000000000001920
        edits_0000000000000001921-0000000000000001922
        edits_0000000000000001923-0000000000000001945
        edits_0000000000000001946-0000000000000001947
        edits_inprogress_0000000000000001948
        fsimage_0000000000000001945
        fsimage_0000000000000001945.md5
        fsimage_0000000000000001947
        fsimage_0000000000000001947.md5
        seen_txid
        VERSION

        1.历史日志文件：编辑完成的日志文件
            日志文件:是记录客户端对元数据操作的日志
            比如：某一个用户对某一个目录执行某一种操作
            edits_0000000000000000001-0000000000000000002
            edits_0000000000000000003-0000000000000000006
            edits_0000000000000000007-0000000000000001813
            edits_0000000000000001814-0000000000000001814
            edits_0000000000000001815-0000000000000001816
            edits_0000000000000001817-0000000000000001821
            edits_0000000000000001822-0000000000000001859
            edits_0000000000000001860-0000000000000001861
            edits_0000000000000001862-0000000000000001863
            edits_0000000000000001864-0000000000000001907
            edits_0000000000000001908-0000000000000001918
            edits_0000000000000001919-0000000000000001920
            edits_0000000000000001921-0000000000000001922
            edits_0000000000000001923-0000000000000001945
            edits_0000000000000001946-0000000000000001947

        2.正在编辑的日志文件：目前对元数据修改的操作记录的文件
            edits_inprogress_0000000000000001948

        3.镜像文件：真实的元数据信息经过序列化之后的文件  在集群启动的时候会加载这个文件，此时会反序列化
            fsimage_0000000000000001945   镜像文件序列化之后的文件
            fsimage_0000000000000001945.md5  镜像文件序列化之后的加密文件
            fsimage_0000000000000001947
            fsimage_0000000000000001947.md5

        4.seen_txid：合并点记录文件   记录的是下一次需要合并的日志文件



        元数据写入:
            真实的硬盘上存储的完整的元数据：fsimage+正在编辑的日志文件
            内存中的元数据是完整的吗？
            无论什么时候内存中保存的元数据永远是最新的最完整的元数据

            如果fsimage不和日志文件进行合并,fsiamge和内存元数据差别越来越大，所以fsimage和日志文件需要定期合并
            这个合并谁在做？
            是secondarynamenode做的
            元数据合并的过程: checkpoint过程
            触发合并的条件:
            1.时间节点   时间间隔3600s = 1h
            <property>
                <name>dfs.namenode.checkpoint.period</name>
                <value>3600</value>
                <description>The number of seconds between two periodic checkpoint
                </description>
            </property>

            2.元数据条数  100万条
            <property>
                <name>dfs namenode checkpoint txns</name>
                <value>1000000</value>
                <description>The Secondary Name Node or CheckpointNode will create a checkpoint
                ace every dfs namenode checkpoint txns transactions, regardless
                dfs namenode, checkpoint period has expired
                </description>
            </property>

            两个触发条件满足任何一个都会触发 checkpoint过程


            secondarynamenode进行checkpoint过程后自己也会保留一份 fsimage文件
            原因是为namenode做备份,以防 namenode宕机元数据丢失的时候帮助进行namenode恢复

            在没有达到checkpoint过程的这段时间集群正常关闭了,在集群关闭之前这个时候内存中元数据会写入磁盘中一份
            关闭集群的时候保证磁盘上的元数据和内存中的一致

namenode的作用
    1.保存元数据
    2.处理客户端的读写请求
    3.负责分配数据块的存储节点
    4.负载均衡
secondarynamenode作用
    1.帮助 namenode做元数据备份帮助 namenode进行恢复
    2.进行 checkpoint,帮助 namenode进行元数据合并。
datanode：
    1.用来存储数据块
    2.处理真正的读写
    3.定期向namenode发送心跳报告（状态，块位置信息）

块的位置信息补充：
    数据块存储在datanode上，每个datanode只知道自己节点上存储了哪些块，并不知道这些块分别属于哪一个文件
    datanode01：blk_02020  blk_92792
    namenode才知道这个快属于哪个文件
    namenode记录磁盘中的元数据信息，不包含数据块存储位置的信息，但是包含文件和数据块的对应关系namenode记录元数据的时候会如下存储：
    hadoop-2.7.6.tar.gz:blk 1:[]
                        blk 2:[]
                        数据块的存储信息会先存为一个空的列表,在 datanode向namenode发送块报告的时候，会把对应块的存储节点添加到列表中


